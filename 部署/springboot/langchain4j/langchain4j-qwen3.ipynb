{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建项目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 新建一个maven 项目，并引入以下依赖："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    <properties>\n",
    "        <spring-boot.version>3.2.6</spring-boot.version>\n",
    "        <knife4j.version>4.3.0</knife4j.version>\n",
    "        <langchain4j.version>1.0.0-beta3</langchain4j.version>\n",
    "    </properties>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    <dependencies>\n",
    "        <!-- web应用程序核心依赖 -->\n",
    "        <dependency>\n",
    "            <groupId>org.springframework.boot</groupId>\n",
    "            <artifactId>spring-boot-starter-web</artifactId>\n",
    "        </dependency>\n",
    "        <!-- 编写和运行测试用例 -->\n",
    "        <dependency>\n",
    "            <groupId>org.springframework.boot</groupId>\n",
    "            <artifactId>spring-boot-starter-test</artifactId>\n",
    "            <scope>test</scope>\n",
    "        </dependency>\n",
    "        <!-- 接入ollama -->\n",
    "        <dependency>\n",
    "            <groupId>dev.langchain4j</groupId>\n",
    "            <artifactId>langchain4j-ollama-spring-boot-starter</artifactId>\n",
    "        </dependency>\n",
    "        <!-- 基于open-ai的langchain4j接口：ChatGPT、deepseek都是open-ai标准下的大模型 -->\n",
    "        <dependency>\n",
    "            <groupId>dev.langchain4j</groupId>\n",
    "            <artifactId>langchain4j-open-ai-spring-boot-starter</artifactId>\n",
    "        </dependency>\n",
    "        <!--langchain4j高级功能-->\n",
    "        <dependency>\n",
    "            <groupId>dev.langchain4j</groupId>\n",
    "            <artifactId>langchain4j-spring-boot-starter</artifactId>\n",
    "        </dependency>\n",
    "    </dependencies>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    <dependencyManagement>\n",
    "        <dependencies>\n",
    "            <!--引入SpringBoot依赖管理清单-->\n",
    "            <dependency>\n",
    "                <groupId>org.springframework.boot</groupId>\n",
    "                <artifactId>spring-boot-dependencies</artifactId>\n",
    "                <version>${spring-boot.version}</version>\n",
    "                <type>pom</type>\n",
    "                <scope>import</scope>\n",
    "            </dependency>\n",
    "\n",
    "            <dependency>\n",
    "                <groupId>dev.langchain4j</groupId>\n",
    "                <artifactId>langchain4j-bom</artifactId>\n",
    "                <version>${langchain4j.version}</version>\n",
    "                <type>pom</type>\n",
    "                <scope>import</scope>\n",
    "            </dependency>\n",
    "            \n",
    "        </dependencies>\n",
    "\n",
    "    </dependencyManagement>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 main函数修改"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@SpringBootApplication\n",
    "public class Main {\n",
    "    public static void main(String[] args) {\n",
    "        SpringApplication.run(Main.class, args);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 application.properties"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "server.port=8080\n",
    "\n",
    "# 远程vllm\n",
    "langchain4j.open-ai.chat-model.api-key=${DEEP_SEEK_API_KEY}\n",
    "langchain4j.open-ai.chat-model.base-url=http://localhost:8000/v1\n",
    "langchain4j.open-ai.chat-model.model-name=Qwen3-8B\n",
    "\n",
    "# 远程ollama\n",
    "#langchain4j.open-ai.chat-model.api-key=${NA_KEY}\n",
    "#langchain4j.open-ai.chat-model.base-url=http://localhost:8000/v1\n",
    "#langchain4j.open-ai.chat-model.model-name=qwen3:8b\n",
    "\n",
    "\n",
    "langchain4j.ollama.chat-model.base-url=http://localhost:11434\n",
    "langchain4j.ollama.chat-model.model-name=qwen3:8b\n",
    "langchain4j.ollama.chat-model.temperature=0.8\n",
    "langchain4j.ollama.chat-model.timeout=PT60S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 在test/java目录下创建com.xllz目录，并创建Qwen3Test.java文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 测试本地ollama的qwen3服务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    @Autowired\n",
    "    private OllamaChatModel ollamaChatModel;\n",
    "    @Test\n",
    "    public void testOllama() {\n",
    "        //思考模式\n",
    "//        String answer = ollamaChatModel.chat(\"你好,你是谁\");\n",
    "\n",
    "        // 禁止思考模式\n",
    "        String answer = ollamaChatModel.chat(\"你好,你是谁 /no_think\");\n",
    "        //输出结果\n",
    "        System.out.println(answer);\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 测试服务器的vllm服务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python -m vllm.entrypoints.openai.api_server --served-model-name Qwen3-8B --model /root/autodl-tmp/Qwen/Qwen3-8B --dtype auto --port 6006 --max_model_len 8784 --gpu_memory_utilization 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    @Autowired\n",
    "    private OpenAiChatModel openAiChatModel;\n",
    "    @Test\n",
    "    public void testSpringBoot() {\n",
    "        //向模型提问\n",
    "        String answer = openAiChatModel.chat(\"你是谁\");\n",
    "\n",
    "        //输出结果\n",
    "        System.out.println(answer);\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 测试服务器ollama服务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ollama serve"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
