server.port=8080

# ??vllm
#langchain4j.open-ai.chat-model.api-key=${NA_KEY}
#langchain4j.open-ai.chat-model.base-url=http://localhost:8000/v1
#langchain4j.open-ai.chat-model.model-name=Qwen3-8B

# ??ollama
langchain4j.open-ai.chat-model.api-key=${NA_KEY}
langchain4j.open-ai.chat-model.base-url=http://localhost:8000/v1
langchain4j.open-ai.chat-model.model-name=qwen3:8b


langchain4j.ollama.chat-model.base-url=http://localhost:11434
langchain4j.ollama.chat-model.model-name=qwen3:8b
langchain4j.ollama.chat-model.temperature=0.8
langchain4j.ollama.chat-model.timeout=PT60S