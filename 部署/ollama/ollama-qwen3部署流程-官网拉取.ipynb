{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source /etc/network_turbo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置ollama环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cpu 加载/ gpu 加载 （自动识别）\n",
    "vim /etc/profile \n",
    "\n",
    "export OLLAMA_HOST=\"0.0.0.0:6006\" \n",
    "\n",
    "export OLLAMA_MODELS=/root/autodl-tmp/models \n",
    "\n",
    "source /etc/profile \n",
    "\n",
    "echo $OLLAMA_HOST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU加载 单卡/多卡\n",
    "\n",
    "vim /etc/profile \n",
    "\n",
    "export OLLAMA_HOST=\"0.0.0.0:6006\" \n",
    "\n",
    "export OLLAMA_GPU_LAYER=cuda\n",
    "\n",
    "export OLLAMA_NUM_GPU=2\n",
    "\n",
    "export CUDA_VISIBLE_DEVICES=0,1\n",
    "\n",
    "export OLLAMA_SCHED_SPREAD=1\n",
    "\n",
    "export OLLAMA_KEEP_ALIVE=-1\n",
    "\n",
    "export OLLAMA_MODELS=/root/autodl-tmp/models \n",
    "\n",
    "source /etc/profile \n",
    "\n",
    "echo $OLLAMA_HOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开启ollama服务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ollama serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从官方拉取模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ollama run qwen3:8b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本地openai调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "嗯，用户问的是“说一下什么是大模型”，首先我需要明确用户的需求。他们可能对这个术语不太了解，想得到一个通俗易懂的解释。作为助理，我要确保回答既全面又不复杂。\n",
      "\n",
      "首先，我要定义大模型是什么，可能用户知道一些AI的概念，但大模型是其中比较新的东西。需要说明大模型的基本概念，比如参数量、训练数据和应用场景。然后，应该提到一些常见的例子，比如GPT、BERT、通义千问，这样用户能更具体地理解。\n",
      "\n",
      "接下来，用户可能想知道为什么大模型重要，所以需要解释它们的优势，比如强大的语言理解和生成能力，多任务处理，还有持续学习的能力。但也要提到挑战，比如计算资源需求高、存在偏见和伦理问题，这样回答会更全面。\n",
      "\n",
      "另外，可能用户对实际应用感兴趣，所以举几个例子会更好，比如聊天机器人、文本生成、代码编写、数据分析等。这样他们能了解大模型在现实中的作用。\n",
      "\n",
      "还要注意避免使用专业术语过多，保持口语化。同时检查是否有遗漏的部分，比如是否提到训练过程或应用场景的多样性。最后，确保结构清晰，分点解释，让用户容易理解。\n",
      "</think>\n",
      "\n",
      "**大模型**（Large Language Models，简称LLM）是指基于人工智能技术，通过海量数据训练出的具有强大语言理解和生成能力的深度学习模型。它们能够处理复杂的自然语言任务，如文本生成、翻译、问答、代码编写等，并且在多个领域展现出强大的泛化能力。\n",
      "\n",
      "---\n",
      "\n",
      "### **核心特点**\n",
      "1. **超大规模参数**  \n",
      "   大模型通常拥有数十亿甚至数千亿个参数（参数是模型内部用于学习数据特征的变量）。参数量越大，模型对语言的理解和生成能力越强。\n",
      "\n",
      "2. **海量数据训练**  \n",
      "   通过训练大量文本数据（如书籍、网页、对话记录等），大模型能学习到语言的结构、语义和上下文关联，从而具备类人水平的语言理解和推理能力。\n",
      "\n",
      "3. **多任务处理能力**  \n",
      "   大模型可以同时完成多种任务，例如：\n",
      "   - 回答开放性问题（如“解释量子力学”）\n",
      "   - 文本生成（如写故事、写邮件）\n",
      "   - 代码编写（如根据需求生成代码）\n",
      "   - 数据分析（如从文本中提取关键信息）\n",
      "\n",
      "4. **持续学习与迭代**  \n",
      "   初期训练完成后，大模型可以通过微调（Fine-tuning）或增量训练进一步优化，适应特定领域或场景（例如医疗、金融、教育等）。\n",
      "\n",
      "---\n",
      "\n",
      "### **典型应用场景**\n",
      "- **自然语言处理（NLP）**：文本摘要、情感分析、机器翻译。\n",
      "- **聊天机器人**：如智能客服、虚拟助手（如Siri、Alexa的升级版）。\n",
      "- **内容创作**：撰写文章、诗歌、剧本，甚至生成代码。\n",
      "- **数据分析与决策支持**：从非结构化数据中提取洞察，辅助商业决策。\n",
      "\n",
      "---\n",
      "\n",
      "### **著名大模型示例**\n",
      "- **GPT系列**（OpenAI）：如GPT-3、GPT-4，以强大的语言生成能力著称。\n",
      "- **BERT**（Google）：专注于理解上下文语义，常用于问答和文本分类。\n",
      "- **通义千问**（Qwen）：阿里巴巴集团旗下的超大规模语言模型，支持多种语言和任务。\n",
      "- **LLaMA/ChatGLM**（Meta、智谱AI等）：开源大模型，适合研究和定制化开发。\n",
      "\n",
      "---\n",
      "\n",
      "### **优势与挑战**\n",
      "- **优势**：\n",
      "  - 强大的语言理解和生成能力。\n",
      "  - 可扩展性高，可适配不同场景。\n",
      "  - 减少对人工规则的依赖。\n",
      "\n",
      "- **挑战**：\n",
      "  - 训练成本高昂（需要大量算力和数据）。\n",
      "  - 存在偏见或错误输出的风险（如生成不准确信息）。\n",
      "  - 对隐私数据的潜在安全隐患。\n",
      "\n",
      "---\n",
      "\n",
      "### **总结**\n",
      "大模型是人工智能领域的重要突破，它们通过模仿人类语言处理能力，正在深刻改变从日常交流到专业工作的方方面面。随着技术进步，未来大模型可能会更高效、更安全，并进一步融入我们的生活。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# client = OpenAI(\n",
    "#     # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n",
    "#     api_key=os.getenv(\"DASHSCOPE_API_KEY\"), # 如何获取API Key：https://help.aliyun.com/zh/model-studio/developer-reference/get-api-key\n",
    "#     base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "# )\n",
    "\n",
    "client = OpenAI(\n",
    "   \n",
    "    api_key=\"na\", \n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    ")\n",
    "\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"midori\", \n",
    "    messages=[\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "        {'role': 'user', 'content': '说一下什么是大模型'}\n",
    "        ]\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通过open-webui 部署ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vim /etc/profile \n",
    "\n",
    "export OLLAMA_HOST=\"0.0.0.0:11434\" \n",
    "\n",
    "source /etc/profile \n",
    "\n",
    "echo $OLLAMA_HOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意，不需要以下环节\n",
    "export HF_ENDPOINT=https://hf-mirror.com \\\n",
    "export ENABLE_OLLAMA_API=False \\\n",
    "export OPENAI_API_BASE_URL=http://127.0.0.1:5000/v1 \\\n",
    "export DEFAULT_MODELS=\"Qwen3-8B\" \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 直接启动open-webui即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ollama serve\n",
    "\n",
    "open-webui serve --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ame",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
